# -*- coding: utf-8 -*-
"""fam_unfam.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x3s-F6oGITQpSpFlI4niOBWw9pqbvxtt
"""

import pandas as pd
import numpy as np
import pickle
import matplotlib.pyplot as plt
from scipy import stats
import tensorflow as tf
import seaborn as sns
from pylab import rcParams
from sklearn.model_selection import train_test_split
from keras.models import Model, load_model
from keras.layers import Input, Dense
from keras.callbacks import ModelCheckpoint, TensorBoard
from keras import regularizers

# %matplotlib inline


RANDOM_SEED = 42
LABELS = ["fam", "unfam"]

df = pd.read_csv("main_combine_1.csv")
df1 = pd.read_csv('main_test_1.csv')

"""# Exploration"""

df.shape
df1.shape

df.isnull().values.any()

unfam= df[df.case == 0]
fam = df[df.case == 1]
famtest = df1[df1.case == 1]

unfam.shape

fam.shape
famtest.shape

"""# Preparing the data"""

from sklearn.preprocessing import StandardScaler

data = df.drop(['Sample'], axis=1)
data1 = df1.drop(['Sample'], axis=1)

X_train, X_test = train_test_split(data, test_size=0.2, shuffle = True, random_state=RANDOM_SEED)

y_train = X_train['case']
X_train = X_train.drop(['case'], axis=1)


y_test = data1['case']
X_test = data1.drop(['case'], axis=1)


X_train = X_train.values
y_train = y_train.values
X_test = X_test.values
y_test = y_test.values

X_train.shape

"""# Building the model"""

input_dim = 4
encoding_dim =36

input_layer = Input(shape=(input_dim, ))

encoder = Dense(encoding_dim, activation="tanh", 
                activity_regularizer=regularizers.l1(10e-5))(input_layer)
encoder = Dense(int(encoding_dim / 2), activation="tanh")(encoder)
encoder = Dense(int(encoding_dim / 4), activation="tanh")(encoder)
decoder = Dense(int(encoding_dim / 4), activation='tanh')(encoder)
decoder = Dense(int(encoding_dim / 2), activation='tanh')(decoder)
decoder = Dense(int(encoding_dim), activation='tanh')(decoder)
decoder = Dense(input_dim, activation='relu')(decoder)

autoencoder = Model(inputs=input_layer, outputs=decoder)

X_test

nb_epoch = 100
batch_size = 256

autoencoder.compile(optimizer='adam', 
                    loss='mean_squared_logarithmic_error', 
                    metrics=['accuracy'])

checkpointer = ModelCheckpoint(filepath="modelfam.h5",
                               verbose=0,
                               save_best_only=True)
tensorboard = TensorBoard(log_dir='./logs',
                          histogram_freq=0,
                          write_graph=True,
                          write_images=True)

history = autoencoder.fit(X_train, X_train,
                    epochs=nb_epoch,
                    batch_size=batch_size,
                    shuffle=True,
                    validation_data=(X_test, X_test),
                    verbose=2,
                    callbacks=[checkpointer, tensorboard]).history

autoencoder = load_model('model.h5')
# a = print(history['loss'])

"""# Evaluation"""

plt.plot(history['loss'])
plt.plot(history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right');

"""The reconstruction error on our training and test data seems to converge nicely. Is it low enough? Let's have a closer look at the error distribution:"""

predictions = autoencoder.predict(X_test)
predictions

mse = np.mean(np.power(X_test - predictions, 2), axis=1)
error_df = pd.DataFrame({'reconstruction_error': mse,
                        'true_class': y_test})
print(mse)